---
title: SOTA
author: "Carlo Rosso"
date: 2025-07-09
---

Some research from ITU shows that transfer language doesn't apply, meaning that
a model trained only in Danish achieves the same accuracy as a model trained in
other languages and fine-tuned in Danish.

[DaNE: A Named Entity Resource for Danish](https://aclanthology.org/2020.lrec-1.565.pdf)
shows that you can take advantage of transfer language, especially from English.
They release a new dataset, and I want to check whether there has been more
study on the topic with the new dataset.

It looks like they appreciate Da-BERT, it seems to be a consistent model.
Multilingual BERT tend to rely a lot on capitalized text.
Finally I understand the kind of analysis I need to go through on the dataset,
to describe more accurately the dataset.

[Neural Cross-Lingual Transfer and LImited Annotated Data for Named Entity
Recognition in Danish](https://aclanthology.org/W19-6143.pdf)
This paper doesn't seem to be interesting compared to the previous (the previous
cite this one and improves it).

This repository seems to be very interesting: [danlp](https://github.com/alexandrainst/danlp).

- https://github.com/alexandrainst/danlp
- https://medium.com/danlp
- https://paperswithcode.com/sota/ner-on-dane

- I should convert the dataset in the CoNLL-U format.

I also find out that there are three different version of DaCy: large, medium
and small. Thus, I think I am gonna build the baseline on the small, which can
be substituted very easly with other models. The reason for this is the speed at
which I am able to verify that everything is working, once I know it, I can put
some danishGPT inside: big and accurate model.

Either way follows:
- [DaCy large](https://dataloop.ai/library/model/chcaa_da_dacy_large_trf) >87% acc on
  DaNE
- [DaCy medium](https://dataloop.ai/library/model/chcaa_da_dacy_medium_trf) >85% acc on
  DaNE
- [DaCy small](https://dataloop.ai/library/model/chcaa_da_dacy_small_trf) >82% acc on
  DaNE

Also seems to be used:
- https://dataloop.ai/library/model/qwen_qwen25-15b-instruct/

Notably, DaCy was published 2 years ago, Qwen only 9 months ago.
DaCy is Danish, Qwen is multilingual, and a paper by KU showed that shouldn't be
a problem.

By the way, DaCy large seems to perform ~1% better than DanishBert on DaNE.

With reference to [DANSK and DaCy 2.6.0](https://arxiv.org/abs/2402.18209),
better models to the previous are:
- [DaCy large](https://huggingface.co/emiltj/da_dacy_large_DANSK_ner) 
- [DaCy medium](https://huggingface.co/emiltj/da_dacy_medium_DANSK_ner)
- [DaCy small](https://huggingface.co/emiltj/da_dacy_small_DANSK_ner) 

Check out this page: [[https://paperswithcode.com/sota/named-entity-recognition-on-dane?p=dacy-a-unified-framework-for-danish-nlp]].

According to the previous page a good model seems to be: [[https://huggingface.co/saattrupdan/nbailab-base-ner-scandi]].
