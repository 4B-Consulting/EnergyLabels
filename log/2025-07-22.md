---
title: Dataset to group tokens part two
author: "Carlo Rosso"
date: 2025-07-22
---

Idea:
I let S_text aside.
To identify the number of the sentence:
1. I split the sentences by '.'
2. I look for each label in each sentence.
    - label is present in a sentence (sentence = label number)
    - label is contained in many sentences (first free sentence = label number)
    - a sentence can have more than one number:
        - when two labels are different
        - when they are contained only in such sentence
    - we also have to distinguish the reason a label has a number: it is
      reasonable that producer and year are to be found in the same sentence.


---

Considering the following tokenizer:

```py
class Tokenizer:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(
            "saattrupdan/nbailab-base-ner-scandi"
        )
        self.model = AutoModelForTokenClassification.from_pretrained(
            "saattrupdan/nbailab-base-ner-scandi"
        )
        self.model.eval()

    def tokenize(self, text):
        """
        Tokenize text and return words, their embeddings, and indices.
        
        Args:
            text (str): Input text to tokenize
            
        Returns:
            tuple: (words, embeddings, indices) where:
                - words: List of word strings
                - embeddings: List of averaged token embeddings for each word
                - indices: List of (start, end) character indices for each word
        """
        # Get tokenized inputs and model outputs
        inputs = self._prepare_inputs(text)
        last_hidden_states = self._get_hidden_states(inputs["input_ids"])
        
        # Extract word information
        word_groups = self._group_tokens_by_word(
            inputs["offset_mapping"][0],
            inputs.word_ids(0),
            last_hidden_states
        )
        
        # Process word groups into final outputs
        words, embeddings, indices = self._process_word_groups(
            word_groups, text
        )
        
        return words, embeddings, indices

    def _prepare_inputs(self, text):
        """Tokenize text and prepare model inputs."""
        return self.tokenizer(
            text,
            return_tensors="pt",
            return_offsets_mapping=True,
            truncation=True,
            max_length=512,
        )

    def _get_hidden_states(self, input_ids):
        """Get last hidden states from the model."""
        with torch.no_grad():
            outputs = self.model(input_ids, output_hidden_states=True)
            return outputs.hidden_states[-1][0]  # Shape: [seq_len, hidden_dim]

    def _group_tokens_by_word(self, offset_mapping, word_ids, hidden_states):
        """Group tokens by their word IDs."""
        word_groups = {}
        
        for idx, word_id in enumerate(word_ids):
            if word_id is None:  # Skip special tokens
                continue
                
            start, end = offset_mapping[idx].tolist()
            
            if word_id not in word_groups:
                word_groups[word_id] = {
                    'embeddings': [],
                    'start': start,
                    'end': end
                }
            
            word_groups[word_id]['embeddings'].append(hidden_states[idx])
            word_groups[word_id]['end'] = end  # Update end position
            
        return word_groups

    def _process_word_groups(self, word_groups, text):
        """Process grouped tokens into words, embeddings, and indices."""
        words = []
        embeddings = []
        indices = []
        
        # Sort by word_id to maintain order
        for word_id in sorted(word_groups.keys()):
            group = word_groups[word_id]
            
            # Extract word text
            start_idx = group['start']
            end_idx = group['end']
            word_text = text[start_idx:end_idx]
            
            # Compute average embedding
            avg_embedding = self._average_embeddings(group['embeddings'])
            
            # Append to results
            words.append(word_text)
            embeddings.append(avg_embedding)
            indices.append((start_idx, end_idx))
            
        return words, embeddings, indices

    def _average_embeddings(self, embeddings_list):
        """Compute average of a list of embedding tensors."""
        return torch.mean(torch.stack(embeddings_list), dim=0)
```

Consider the following function:

```py
def ner_data(file_name="data/data_district_heating.xlsx"):
    tokenizer = Tokenizer()

    labels = [
        "None",
        "Pieces",
        "Manufacturer",
        "SubType",
        "HxType",
        "NominalEffectEach",
        "Year",
    ]

    symbols = [".", "i", "er", "veksler", "et", "varme", "var", "af"]
    labels_to_num = np.eye(7)
    d_gt = pd.read_excel(file_name, sheet_name="Ground Truth")
    # %%
    pieces = (
        d_gt.iloc[:, [7, 13, 19, 25]]
        .fillna("")
        .astype(str)
        .agg(" ".join, axis=1)
        .str.lower()
    )
    manufacturers = (
        d_gt.iloc[:, [8, 14, 20, 26]]
        .fillna("")
        .astype(str)
        .agg(" ".join, axis=1)
        .str.lower()
    )
    subtype = (
        d_gt.iloc[:, [9, 15, 21, 27]]
        .fillna("")
        .astype(str)
        .agg(" ".join, axis=1)
        .str.lower()
    )
    hxType = (
        d_gt.iloc[:, [10, 16, 22, 28]]
        .fillna("")
        .astype(str)
        .agg(" ".join, axis=1)
        .str.lower()
    )
    nominalEffectEach = (
        d_gt.iloc[:, [11, 17, 23, 29]]
        .fillna("")
        .astype(str)
        .agg(" ".join, axis=1)
        .str.lower()
    )
    year = (
        d_gt.iloc[:, [12, 18, 24, 30]]
        .fillna("")
        .astype(str)
        .agg(" ".join, axis=1)
        .str.lower()
    )
    # %%
    ground_truth = pd.DataFrame()
    ground_truth["text"] = (
        d_gt[["S_text", "L_text"]].fillna("").astype(str).agg(" ".join, axis=1)
    )
    # %%
    ground_truth["words"] = pd.NA
    ground_truth["tokens"] = pd.NA
    ground_truth["labels"] = pd.NA

    for i in trange(len(ground_truth)):
        words, tokens, _ = tokenizer.tokenize(ground_truth.loc[i, "text"])
        words = [word.lower() for word in words]
        ground_truth.at[i, "words"] = words
        ground_truth.at[i, "tokens"] = tokens

        labels = labels_to_num[0] * np.ones((len(words), 7))

        for j, word in enumerate(words):
            if contains(word, symbols):
                continue
            elif contains(word, pieces[i]):
                labels[j] = labels_to_num[1]
            elif contains(word, manufacturers[i]):
                labels[j] = labels_to_num[2]
            elif contains(word, subtype[i]):
                labels[j] = labels_to_num[3]
            elif contains(word, hxType[i]):
                labels[j] = labels_to_num[4]
            elif contains(word, nominalEffectEach[i]):
                labels[j] = labels_to_num[5]
            elif contains(word, year[i]):
                labels[j] = labels_to_num[6]

        ground_truth.at[i, "labels"] = labels

    return ground_truth
```

Implement a new function similar to `ner_data`, but it should work in the
opposite way: instead of looking for the words in the target results, it should
look for the results in the sample. Whenever a sentence contains such result, we
can label such sentence. The sentences labels are:
["0", "1", "2", "3", "4", "any"]
"0" means that such sentence is not describing any heat exchanger.
"any" means that such sentence is describing more than one heat exchanger.
Let S_text aside.
To identify the number of the sentence:
1. Split the sentences by '.'
2. Look for each label in each sentence.
    - when label is present in a sentence, sentence number = label number
    - when label is contained in many sentences, first free sentence with no label = label number
    - for any to apply two words with different label number needs to be in such sentence
    - a sentence can have more than one label:
        - when two labels are different (
        - when they are contained only in such sentence
    - we also have to distinguish the reason a label has a number: it is
      reasonable that producer and year are to be found in the same sentence.

