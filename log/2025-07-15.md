---
title: "<Log>"
author: "Carlo Rosso"
date: 2025-07-15
---

1. tokenize the text
2. apply pca on each token
3. small neural network to identify entities
    - neural network
    - what about cnn? what interpretation does it have?
4. Check whether the answer is correct with a chatbot.
5. Save a good result in a dataframe
6. Repeat in batch with random extraction.

--- 

1. tokenize the text
2. neural network to identify the class of each token
3. check whether the answer is correct with a chatbot.
5. save a good result in a dataframe.
6. repeat in batch with random extraction.

---

1. extract random samples' batch
2. for each sample, tokenize the description by word
3. neural network on each token to identify its class: (Pieces, Manufacturer,
   SubType, ...)
4. check whether the tagging of each sample is correct with a chatbot.
5. save good result in a Dataframe, for future extractions.
6. repeat in batch with random extraction.


first prompt to describe the task:

```
Example 1:
        Input: "Fjernvarme med isoleret veksler (indirekte anlæg) - nyere. 
        Bygningen opvarmes med fjernvarme. 
        Anlægget er udført med isoleret varmeveksler og indirekte centralvarmevand i fordelingsnettet. 
        Anlægget er opstillet i Badensgade 41."
        Output: {{
            "Pieces": 1,
            "Manufacturer": "Unknown",
            "SubType": null,
            "HxType": "Eksisterende fjernvarme",
            "NominalEffectEach": null,
            "Year": null
        }}

        Example 2:
        Input: "Fjernvarme med isoleret veksler (indirekte anlæg) - nyere. Ejendommen opvarmes med fjernvarme fra HOFOR.
    Til opvarmning af radiatorerne er der 1 stk. isoleret varmevekslere monteret i fjernvarmeunit,  fabrikat Redan.
    Fjernvarmeunit er placeret i kælderen."
        Output: {{
            "Pieces": 1,
            "Manufacturer": "Danfoss Redan",
            "SubType": null,
            "HxType": "Isoleret varmeveksler",
            "NominalEffectEach": null,
            "Year": null
        }}

        Example 3:
        Input: "Fjv. Installation efter 1980 (isoleret). Ejendommen opvarmes med indirekte fjernvarme. 
    Bygningen opvarmes med fjernvarme med veksler.
    Veksleren er af fabrikat WPH, type SL70TL-1-90CC fra 2008 og vurderes isoleret med 40 mm PUR.
    Jf. tidligere energimærkerapport er der brændeovne i de enkelte boliger.
    I værkstedet i baghuset mod nordvest er der monteret en lille elradiator. Rummet er ikke medtaget som opvarmet i beregningen, da rummet alene vurderes kortvarigt opvarmet."
        Output: {{
            "Pieces": 1,
            "Manufacturer": "WPH Teknik",
            "SubType": SL70TL-1-90CC,
            "HxType": "Isoleret varmeveksler",
            "NominalEffectEach": null,
            "Year": 2008
        }}

        Example 4:
        Input: "Fjernvarme med isoleret veksler (indirekte anlæg) - efter 1980. Bygningen opvarmes med fjernvarme.
    Anlægget er udført med isoleret varmeveksler og indirekte centralvarmevand i fordelingsnettet.
    I teknikrum er opstillet 2 stk. varmevekslere - 1 stk. fabr. Sondex type ST 15-ST (radiatorer) à 200 kW og 1 stk. Sondex type ST 20-ST (ventilationsvarmeflader) à 180 kW.
    Vekslere er præisolerede."
        Output: {{
            "Pieces": 1,
            "Manufacturer": "Sondex Teknik",
            "SubType": "ST 15-ST",
            "HxType": "Isoleret varmeveksler",
            "NominalEffectEach": "200 kW",
            "Year": "After 1980"
        }}

        You are provided a sentence, and you have to extract the following values:
            - pieces: the quantity of heating systems.
            - manufacturer: if it is not specified it is Unknown
            - SubType: must be id to identify the model, e.g. null, SL3323TLX, SKR, APVB,
            - HxType: Heat Exchanger Type, e.g. Isoleret varmeveksler, Fjernvarmeveksler, Varmeveksler
            - NominalEffectEach, e.g. 1100 kW, 150 kW, 400 kW
            - Year: e.g. After 1980, 2017, 2000-2009
        Return a JSON with the values.
Input: "Fjernvarme - isoleret varmeveksler - indirekte anlæg.  .  Bygningen opvarmes med fjernvarme. Anlægget er udført med en isoleret pladevarmeveksler, fabrikat Reci fra 2014 og indirekte centralvarmevand i fordelingsnettet. Effekten for veksleren beregnes til 25W/m2, da den ikke kunne aflæses på typeskiltet på veksleren.

Der fortages regelmæssig (årligt) service af ejendommens varmecentral, herunder rens af varmtvandsbeholder o.l."
Oputput:
```

Second prompt to ask for the model:

```
I am tackling the task to extract such labels from text. I am building a pipeline, firstly I want to build a NER, that classify each word. Can you help me build the following model:

1. extract random samples' batch
2. for each sample, tokenize the description by word
3. neural network on each token to identify its class: (Pieces, Manufacturer, SubType, ...)
4. check whether the tagging of each sample is correct with an llm 
5. save good result in a Dataframe: we don't need the chatbot to check on them
   anymore
6. repeat in batch with random extraction.

I already implemented the llm, it has the following interface:
- it takes in a sentence, and the tagged sentence on each word that belongs to one of the interesting labels
- it returns 'True' when the sentence is completely and accurately tagged
- otherwise it returns 'False'

Help me also implementing the model, and the training routine. 
Basically, the model get a string, that corresponds to a word.
It uses an llm tokenizer fine-tuned in danish, to create meaningful embeddings.
I want to use a fast tokenizer, for example DaCy or `saattrupdan/nbailab-base-ner-scandi`.
Each embedding is given to a small neural network. The neural network return an 
array, in which each entry reprensents the
probability that such word belongs to each label ("None", "Pieces",
"Manufacturer", ...). Effectively, when a word is composed of multiple tokens,
we return compute the average of the the result of each token, and we use such
result as the class prediction for such word.
Then the sentence is reassembled, in such a way that every word is tagged
approapriately:
"hello world, Reci" -> {hello world, {Reci}_{Manufacturer}", of course the tag
"None" can be ignored.
we evaluate the sentence with the llm. if the llm returns the same sentence,
we assign loss of 0 to such prediction, otherwise, we assign loss of 1 to all
the predictions of such sentence.
```

I can improve the above model:
- recurrent neural network to mix the tokens corresponding to the same word
- llm to check on the correctness of each word: it return the correct
  prediction, if it is unchanged than the loss is 0, otherwise the loss for each
  word is computed. This allow for better loss.
