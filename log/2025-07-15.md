---
title: "<Log>"
author: "Carlo Rosso"
date: 2025-07-15
---

1. tokenize the text
2. apply pca on each token
3. small neural network to identify entities
    - neural network
    - what about cnn? what interpretation does it have?
4. Check whether the answer is correct with a chatbot.
5. Save a good result in a dataframe
6. Repeat in batch with random extraction.

--- 

1. tokenize the text
2. neural network to identify the class of each token
3. check whether the answer is correct with a chatbot.
5. save a good result in a dataframe.
6. repeat in batch with random extraction.

---

1. extract random samples' batch
2. for each sample, tokenize the description by word
3. neural network on each token to identify its class: (Pieces, Manufacturer,
   SubType, ...)
4. check whether the tagging of each sample is correct with a chatbot.
5. save good result in a Dataframe, for future extractions.
6. repeat in batch with random extraction.


First prompt to describe the task:

```
Example 1:
        Input: "Fjernvarme med isoleret veksler (indirekte anlæg) - nyere. 
        Bygningen opvarmes med fjernvarme. 
        Anlægget er udført med isoleret varmeveksler og indirekte centralvarmevand i fordelingsnettet. 
        Anlægget er opstillet i Badensgade 41."
        Output: {{
            "Pieces": 1,
            "Manufacturer": "Unknown",
            "SubType": null,
            "HxType": "Eksisterende fjernvarme",
            "NominalEffectEach": null,
            "Year": null
        }}

        Example 2:
        Input: "Fjernvarme med isoleret veksler (indirekte anlæg) - nyere. Ejendommen opvarmes med fjernvarme fra HOFOR.
    Til opvarmning af radiatorerne er der 1 stk. isoleret varmevekslere monteret i fjernvarmeunit,  fabrikat Redan.
    Fjernvarmeunit er placeret i kælderen."
        Output: {{
            "Pieces": 1,
            "Manufacturer": "Danfoss Redan",
            "SubType": null,
            "HxType": "Isoleret varmeveksler",
            "NominalEffectEach": null,
            "Year": null
        }}

        Example 3:
        Input: "Fjv. Installation efter 1980 (isoleret). Ejendommen opvarmes med indirekte fjernvarme. 
    Bygningen opvarmes med fjernvarme med veksler.
    Veksleren er af fabrikat WPH, type SL70TL-1-90CC fra 2008 og vurderes isoleret med 40 mm PUR.
    Jf. tidligere energimærkerapport er der brændeovne i de enkelte boliger.
    I værkstedet i baghuset mod nordvest er der monteret en lille elradiator. Rummet er ikke medtaget som opvarmet i beregningen, da rummet alene vurderes kortvarigt opvarmet."
        Output: {{
            "Pieces": 1,
            "Manufacturer": "WPH Teknik",
            "SubType": SL70TL-1-90CC,
            "HxType": "Isoleret varmeveksler",
            "NominalEffectEach": null,
            "Year": 2008
        }}

        Example 4:
        Input: "Fjernvarme med isoleret veksler (indirekte anlæg) - efter 1980. Bygningen opvarmes med fjernvarme.
    Anlægget er udført med isoleret varmeveksler og indirekte centralvarmevand i fordelingsnettet.
    I teknikrum er opstillet 2 stk. varmevekslere - 1 stk. fabr. Sondex type ST 15-ST (radiatorer) à 200 kW og 1 stk. Sondex type ST 20-ST (ventilationsvarmeflader) à 180 kW.
    Vekslere er præisolerede."
        Output: {{
            "Pieces": 1,
            "Manufacturer": "Sondex Teknik",
            "SubType": "ST 15-ST",
            "HxType": "Isoleret varmeveksler",
            "NominalEffectEach": "200 kW",
            "Year": "After 1980"
        }}

        You are provided a sentence, and you have to extract the following values:
            - pieces: the quantity of heating systems.
            - manufacturer: if it is not specified it is Unknown
            - SubType: must be id to identify the model, e.g. null, SL3323TLX, SKR, APVB,
            - HxType: Heat Exchanger Type, e.g. Isoleret varmeveksler, Fjernvarmeveksler, Varmeveksler
            - NominalEffectEach, e.g. 1100 kW, 150 kW, 400 kW
            - Year: e.g. After 1980, 2017, 2000-2009
        Return a JSON with the values.
Input: "Fjernvarme - isoleret varmeveksler - indirekte anlæg.  .  Bygningen opvarmes med fjernvarme. Anlægget er udført med en isoleret pladevarmeveksler, fabrikat Reci fra 2014 og indirekte centralvarmevand i fordelingsnettet. Effekten for veksleren beregnes til 25W/m2, da den ikke kunne aflæses på typeskiltet på veksleren.

Der fortages regelmæssig (årligt) service af ejendommens varmecentral, herunder rens af varmtvandsbeholder o.l."
Oputput:
```

Second prompt to describe the neural network:

```
I am tackling the task to extract such labels from text. 
I want to build a NER, that classify each word. 
Follows the outline of the pipeline:

1. extract random samples' batch
2. for each sample, tokenize the description by word, and compute the embedding
3. neural network on each token to identify its class: (Pieces, Manufacturer, SubType, ...)
4. check whether the tagging of each sample is correct with an llm 
5. save good result in a Dataframe: we don't need the chatbot to check on them
   anymore
6. repeat in batch with random extraction.

Consider that only the neural network to classify each token is missing.
The neural network takes as an input an embedding of a word, and it returns the 
logits for each of the following class: None, Pieces, Manufacturer, SubType, 
HxType, NominalEffectEach, Year. The neural network must be flexible, 
because I want to change the tokenizer in the future, thus it have to have as an 
input the size of the embeddings.
The context is not needed, because the way I compute each embedding already
is influenced by the context.
Implement the neural network and only the neural network.
```

Third prompt to describe the tokenizer:

```
I am tackling the task to extract such labels from text. 
I want to build a NER, that classify each word. 
Follows the outline of the pipeline:

1. extract random samples' batch
2. for each sample, tokenize the description by word, and compute the embedding
3. neural network on each token to identify its class: (Pieces, Manufacturer, SubType, ...)
4. check whether the tagging of each sample is correct with an llm 
5. save good result in a Dataframe: we don't need the chatbot to check on them
   anymore
6. repeat in batch with random extraction.

Consider that only the class `Tokenizer` to build the embeddings is missing.
- Such class have available the function `tokenize` that takes as an input a sample, thus multiple sentences.
- `tokenize` returns the last representation of the tokens, thus the last hidden state of the model, and the word that makes the token.
- I want to use `saattrupdan/nbailab-base-ner-scandi` to compute the tokens of the input
- I want to use the definition of word defined by such tokenizer
- All tokens corresponding to the same word are grouped and averaged

Implement the class `Tokenizer` and only the class `Tokenizer`
```

Prompt to recompose tagged sentence into labels:

```
I am tackling the task to extract such labels from text. 
I want to build a NER, that classify each word. 
Follows the outline of the pipeline:

1. extract random samples' batch
2. for each sample, tokenize the description by word, and compute the embedding
3. neural network on each token to identify its class: (Pieces, Manufacturer, SubType, ...)
4. check whether the tagging of each sample is correct with an llm 
5. save good result in a Dataframe: we don't need the chatbot to check on them
   anymore
6. repeat in batch with random extraction.

The implemented tokenizer given a sample as the follwing:
"Fjernvarme med isoleret veksler (indirekte anlæg) - nyere. Bygningen opvarmes
med fjernvarme. Anlægget er udført med isoleret varmeveksler og indirekte
centralvarmevand i fordelingsnettet. Anlægget er opstillet i Badensgade 41."

Returns the words:
1. "['Fjernvarme', 'med', 'isoleret', 'veksler', '(', 'indirekte', 'anlæg', ')', '-', 'nyere', '.', 'Bygningen', 'opvarmes', 'med', 'fjernvarme', '.', 'Anlægget', 'er', 'udført', 'med', 'isoleret', 'varmeveksler', 'og', 'indirekte', 'centralvarmevand', 'i', 'fordelingsnettet', '.', 'Anlægget', 'er', 'opstillet', 'i', 'Badensgade', '41', '.']"
2. the embedding of each token.

The tokenizer is already implemented
You need to implement a function that given a tagged sentence such as:
sample: "hello world, producer {Reci}_{Manufacturer} in {2008}_{Year}".
- Extracts the tags for each word. 
- The result should be an array of the lenghts of the words of the tagger. 
- Each entry is an array of logits for each label.
- The labels are {None, Manufacturer, SubType, HxType, ...}
Implement this function and only this function

Not to mix things up, you should find the first word of the list of words in the
tagged sentence, you identify it's tag and you append an element to the list of
tags if the tag is none the element will have value [1, 0, 0, 0, 0, 0, 0], and
then you keep going with the next word.
Can you improve the function: initialize the resulting array into an array of the size of tokenized_words, with values of None, namely [1, 0, 0, 0, 0, 0, 0]. If a word is tagged than you change the value for such element. use numpy arrays for it.
```

And then:

```
Sounds perfect, can you do also the opposite: you have the actual sentence, the tokens and the tags for each token, can you produce the tagged sentence?
```

Final prompt to put everything together:
```
I am tackling the task to extract such labels from text. 
I want to build a NER, that classify each word. 
Follows the outline of the pipeline:

1. extract random samples' batch
2. for each sample, tokenize the description by word, and compute the embedding
3. neural network on each token to identify its class: (Pieces, Manufacturer, SubType, ...)
4. check whether the tagging of each sample is correct with an llm 
5. save good result in a Dataframe: we don't need the chatbot to check on them
   anymore
6. repeat in batch with random extraction.

I built the tokenizer that works as follow:
- takes in a sentece
- returns
    - array of strings corresponding to words
    - one token for each word (it has some context influence)

I built the neural netword `TokenClassifier` for class prediction, such that given an array of tokens, for each of them it returns an array of logits for the classes [None, Pieces, Manufacturer, and so on].

I implemented a chatbot `chatbot`, basically it is a class. When you initialize it you
need to give it a layout prompt function, meaning a function that takes in a
string and returns the prompt.
Then it has a method `query`, where you give some text and it returns the
answer.

I implemented `extract_tags` that takes as input a tagged sentence and the list of words, returns the same array that is predicted by the neural network.

I implemented `tag_sentece` that takes in input the list of words, and the tags
of each and returns the tagged string.

Can you put everything together to train the neural network. In particular I
want to store the tokenized sentence, meaning both tokens and list of words
every time they are computed, to avoid to compute them twice.
I also want to store the class prediction of the tokens when the neural network
and the large language model agrees, and only when both of them agrees.
```
