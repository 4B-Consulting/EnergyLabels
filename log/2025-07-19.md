---
title: Ground Truth for NER NN
author: "Carlo Rosso"
date: 2025-07-19
---

I have not been able to think of a way to evaluate the NER neural network
results with an LLM, thus I switch to a hard-match way.

In [[../scripts/NER_ground_truth.py]] I built a program that builds the dataset
for the NER NN from the ground truth of the original dataset:
1. computes the words of each sample
2. computes the tokens of each word
3. computes the ground truth as an array:
    -`None`: [1, 0, 0, 0, 0, 0, 0]
    -`Pieces`: [0, 1, 0, 0, 0, 0, 0]
    -`Manufacturer`: [0, 0, 1, 0, 0, 0, 0]
    -`SubType`: [0, 0, 0, 1, 0, 0, 0]
    -`HxType`: [0, 0, 0, 0, 1, 0, 0]
    -`NominalEffectEach`: [0, 0, 0, 0, 0, 1, 0]
    -`Year`: [0, 0, 0, 0, 0, 0, 1]

Finally, it stores the ground truths in a file in the folder `data`.
Apparently, it is quite a big file, and it might not be worth it to keep the
whole thing in memory: I got 2min and 30s to compute it, thus, I think that
tokenize the sentences is not heavy on the cpu, and I do not need to save the
tokenization on a file. On the other hand, I would still prefer to tokenize
every sentence, compared to tokenize each sentence at prediction time.

Thus, follows the prompt to train the neural network:

I have a neural network:
```
import torch.nn as nn
import torch.nn.functional as F


class TokenClassifier(nn.Module):
    def __init__(self, embedding_dim, hidden_dim=49, num_classes=7):
        """
        Args:
            embedding_dim: Dimension of input token embeddings
            hidden_dim: Size of hidden layer (0 for no hidden layer)
            num_classes: Number of output classes (default=7)
        """
        super().__init__()
        self.num_classes = num_classes
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.fc2 = nn.ReLU(hidden_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, num_classes)

    def forward(self, token_embeddings):
        """
        Args:
            token_embeddings: Tensor of shape (batch_size, seq_len, embedding_dim)
        Returns:
            logits: Tensor of shape (batch_size, seq_len, num_classes)
        """
        batch_size, seq_len, emb_dim = token_embeddings.shape
        flat_embeddings = token_embeddings.view(-1, emb_dim)
        x = F.relu(self.fc1(flat_embeddings))
        x = F.relu(self.fc2(x))
        flat_logits = self.output(x)
        logits = flat_logits.view(batch_size, seq_len, self.num_classes)
        return logits
```

Then I have a database:
df['tokens', 'labels'], where tokens are bert tokens, thus they have bert size,
and labels is an array of the size of tokens, where each entry is an array of 
size 7, an entry for each class.
Can you write the pipeline to train the neural network to classify the tokens?
Be aware that different samples can have different number of tokens and the
number of len(tokens) == len(labels) is always True.
Train the model in batch, improve the size of the batch when the loss reaches 0.
Keep some piece of the dataset for evaluation. Evaluate every 50 epochs.
The fit function should return: loss on training every 50 epochs, loss on
evaluation set every 50 epochs.

In this way, it looks like I managed to train NER Neural Network.
Now I need to come up with an idea to take advantage of the tagging of NER NN to
get some result in between or the final result.

-[x] I moved [`TokenClassifier](../scripts/NERmodel.py) intro its own script.
-[ ] Implementing function to show the precision of the model

```
define a funtion that computes:
- precision
- recall
- F1-score
- confusion matrix

provided:
- predicted labels: an integer (in this case from 0 to 7, but it should be
  flexible)
- expected labels: an integer as above
```

Second classifier problem:
building dataset: some entries might be the same for heater one and heater two,
thus, I need to figure out a way to classify the token appropriately.
I need to understand how tokens are classified, and miss-classified this far, and
then I can figure out a way to classify them appropriately.

Solution:
- check with a program the entries where there is the same number of pieces/same
  manufacturer, when it occurs then I classify those samples by hand, hopefully
  they won't be too many.
