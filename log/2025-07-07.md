---
title: Planning
author: "Carlo Rosso"
date: 2025-07-07
---

# Planning

I am going to work for 3 weeks + 2/3 days. Follows the draft of the planning:

1. Theoretical studies of the problem:
    - What does the problem consist in?
    - How can it be tackled?
    - Are there similar projects available?
    - Are there similar dataset available?
2. Study on the dataset:
    - How many entries are there.
    - What is the average length of the samples.
    - How many labels are there.
    - What are the labels.
    - How many entries have labels.
    - How good are the labels (I see a lot of unknown, what does it mean).
3. Implementing the baseline:
    - Implement a model that is able to classify the dataset.
    - Firstly apply it to a similar dataset to validate the model.
    - Apply the baseline to given dataset, which gives 0.1 version of predicted labels.
4. Theoretical studies of the best approach:
    - Find state-of-the-art solutions.
    - Reading the literature.
    - Write down ideas to improve the state-of-the-art.
5. Write down the report of the work done so far.

This draft is meant to be refined as I tackle the task.

## Week 1

Task 1 and 2. About 5 days and 3 days, respectively, they are meant to be worked on together.
For example to find similar dataset, I need to understand the current dataset.

## Week 2

Task 2 and 3. 2 days and 5 days, respectively, they are meant to be worked on
together. In week 2 the analysis on the dataset is focused on the understanding
of the dataset to find relevant resources. In this case, the study of the
dataset is meant to identify the best model to tackle the task.

## Week 3

Once the problem is understood I can go back to the theoretical studies to
find other approach to improve performances. Basically, the baseline is to have
a starting point to improve the performances.

## Last 2/3 days

Review the work done so far, go through the log and write down:
- What is the problem.
- What is the dataset.
- What is the baseline.
- What are similar approaches.
- Ideas to improve the baseline.

# Dataset

I am studying the dataset.

## Labels - Y

I notice that the labels are for times the following:
- `Pieces`: number
- `Manufacturer`: string
- `SubType`: string
- `HxType`: string
- `NominelEffectEach`: string
- `Year1`: number / string

So I can subdivide the task in two:
1. Model that split the input for each of the 4 piece:
    - Model to count the number of pieces;
    - Model to decide where the split is applied.
2. Model that extract the information for each of the pieces.

This has the advantage to create a more specialized model on task 2, plus it
augment the dimensions of the dataset.

About point 2, check whether more models are required (i.e. one for each label
entry, or a single model that extract everything).

## Features - X

There are two features that are interesting to predict the information:

- `S_text`: short string
- `L_text`: long string

Effectively neither is too long.

# Problem

Multi-Instance Information Extraction (MIIE) is the task of automatically 
identifying and extracting multiple distinct occurrences of a structured entity 
(or "instance") from unstructured or semi-structured text, where each instance 
consists of interrelated attributes that must be correctly grouped.

## Approach

Defining the problem, I realize that I cannot split the input in 4, because the
division of the information might not be uniform, and the same sentence could
contain mixed information about multiple heat exchanger.

I can try following approach:

1. Named Entity Recognition (NER): Use a single model to tag all relevant spans
   in the text (M: MANUFACTURER, S: SUBTYPE, E: EFFECT, Y: YEAR, P: PIECES, H: HXTYPE).
2. Cluster Entities into Instances
    - Rule-based grouping: Entities appearing in the same sentence/phrase goes in the same instance.
    - ML-based grouping: Train a classifier to link entities .
3. Map to Fixed Output Structure, for example through REGEX or something.
Fill up to 4 instances (pad with NaN if <4 or something else).

- Check out the performance of rule-based grouping: effectively I need to
  prepare the dataset for it. And so I need to tag `S_text` and `L_text`.

I think that having 2 models (point 1 and 2) is preferable to a single model,
because I don't have much hardware, and smaller more specific models are able to
tackle a portion of the big problem more precisely.

- Find similar datasets and projects.
- Consider the kind of processing needed on the dataset.
    - How should I change the input? 
    - How should I change the output? 
    - Do I need to prepare some intermediate dataset? Is it feasible?
