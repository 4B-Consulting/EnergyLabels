---
title: "<Log>"
author: "Carlo Rosso"
date: 2025-07-12
---

There are 8745 sentences, and there are 8662 labels, so there are 0.99 labels in
each sentence, almost one label per sentence.

I wanted to tag the text in such a way that each element has a tag, but
apparently find and replace is not a viable solution: 
- pieces has value 1 (and not only)
- the manufacturer is Unknown
- "efter 1980" become "after 1980"/"f√∏r 1970" become "before 1970"

Still sometimes the tagging is successful and it extracts relevant information.
Thus, I would build a pipeline that does not need the tagger.
Then, I can train another model that tags the text, and then I retrain the
previous model on the tagged text and check out how much the accuracy improves,
to see whether it is helpful.

I built a chatbot that classify the entries. I think that this approach is the
most flexible, and the pipeline I would build would keep the chatbot as final
refiner. The chatbot doesn't need to be fine-tuned, so it can be used to check
the results, basically as a final refiner. The fine-tuning you can do with the
chatbot regards the prompt.

Errors on the following entries:

```
errors = [35, 73, 232, 248, 414, 431, 432, 761, 762, 763, 801, 823, 887, 892, 979, 1087, 1159, 1196, 1397, 1507, 1574, 1587, 1590, 1642, 1685, 1689, 1702, 1727, 1762, 1781, 1823, 1986, 2015, 2016, 2022, 2118, 2133, 2182, 2194, 2204]
```

They are 40.
