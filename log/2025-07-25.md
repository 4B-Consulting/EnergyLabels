---
title: "<Log>"
author: "Carlo Rosso"
date: 2025-07-25
---

The accuracy with the previous model was so low, that I changed model into:
[Phi 3.1 mini 4k](https://huggingface.co/bartowski/Phi-3.1-mini-4k-instruct-GGUF)

The input is some text, I split it at the point. And I label it in the following
classes:
- `0`: no reference to any exchanger
- `1`: reference to exchanger 1
- `2`: reference to exchanger 2
- `3`: reference to exchanger 3
- `4`: reference to exchanger 4
- `multiple`: reference to more than one exchanger

Thus, if a sample has only 0 and 1, I can assume pd.NA for entries of exchangers
2, 3, 4. I can check whether the dataset I obtain is consistent.

I hope it is.
How can I build the model to predict these classes:
- I want to try the token model I already implemented
    - I classify each word
    - I average them and I get the value
    - Otherwise I take the word with the maximum value, and I discard the rest.
- Something with more context could be better
    - BERT to classify each sentence: `AutoModelForClassification` (or something)
    - It doesn't have the context of the sentence in the sentences, which might
      be very relevant.
    - I can look for something with a bigger context window.

---

Little presentation on the work done:
- go through the notes
- identify what and resume it
- add why, and how, briefly
- it should be around 10 min
